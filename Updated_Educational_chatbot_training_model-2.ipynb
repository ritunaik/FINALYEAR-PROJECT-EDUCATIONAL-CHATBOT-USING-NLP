{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cec517e6"
      },
      "source": [
        "# Educational Assistant Chatbot\n",
        "\n"
      ],
      "id": "cec517e6"
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request\n"
      ],
      "metadata": {
        "id": "NMYzQv6lpLAm"
      },
      "id": "NMYzQv6lpLAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mjRHqSA8b7C"
      },
      "source": [
        "![download.png](data:image/png;base64,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)"
      ],
      "id": "6mjRHqSA8b7C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75cb48e6"
      },
      "source": [
        "## **Import and load the data file**"
      ],
      "id": "75cb48e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7f5163a"
      },
      "source": [
        "We import the necessary packages for our chatbot and initialize the variables we will use in our Python project.\n",
        "The data file is in JSON format so we used the json package to parse the JSON file into Python."
      ],
      "id": "d7f5163a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d91f7ee",
        "outputId": "32074213-f7a0-43ae-d779-b84169100bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')#Sentence tokenizer"
      ],
      "id": "0d91f7ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3775770"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "d3775770"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "577ce419"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random"
      ],
      "id": "577ce419"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4164ac69"
      },
      "source": [
        "# **Preprocessing**"
      ],
      "id": "4164ac69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqw2SRga7sfJ",
        "outputId": "be53320f-bfe3-4029-ad49-4d038f06b0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "kqw2SRga7sfJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVbx4sDY2rjc"
      },
      "outputs": [],
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents (1).json').read() # read json file\n",
        "intents = json.loads(data_file) # load json file"
      ],
      "id": "nVbx4sDY2rjc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03092c9"
      },
      "source": [
        "When working with text data, we need to perform various preprocessing on the data before we make a machine learning or a deep learning model. Based on the requirements we need to apply various operations to preprocess the\n",
        "data.\n",
        "- Tokenizing is the most basic and first thing you can do on text data.\n",
        "- Tokenizing is the process of breaking the whole text into small parts like words.\n",
        "- Here we iterate through the patterns and tokenize the sentence using nltk.word_tokenize() function and append each word in the words list. We also create a list of classes for our tags."
      ],
      "id": "b03092c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "636dbf65"
      },
      "outputs": [],
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)# add each elements into list\n",
        "        #combination between patterns and intents\n",
        "        documents.append((w, intent['tag']))#add single element into end of list\n",
        "        # add to tag in our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "id": "636dbf65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c496469",
        "outputId": "b4043b9f-2649-4cc9-cd8c-da748cfee905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "nltk.download('wordnet') #lexical database for the English language"
      ],
      "id": "6c496469"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8826e696",
        "outputId": "5c2f458a-cc86-424a-e235-8d791d8a5053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "id": "8826e696"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33220daa"
      },
      "source": [
        "Now we will lemmatize each word and remove duplicate words from the list.\n",
        "- Lemmatizing is the process of converting a word into its lemma form and then creating a pickle file to store the Python objects which we will use while predicting."
      ],
      "id": "33220daa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea121ce6",
        "outputId": "4b2b316a-a504-489e-d517-019df226cf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "405 documents\n",
            " [(['Hi'], 'greeting'), (['How', 'are', 'you', '?'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['What', \"'s\", 'up'], 'greeting'), (['how', 'are', 'ya'], 'greeting'), (['heyy'], 'greeting'), (['whatsup'], 'greeting'), (['?', '?', '?', '?', '?', '?', '?', '?'], 'greeting'), (['cya'], 'goodbye'), (['see', 'you'], 'goodbye'), (['bye', 'bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['I', 'am', 'Leaving'], 'goodbye'), (['Bye'], 'goodbye'), (['Have', 'a', 'Good', 'day'], 'goodbye'), (['talk', 'to', 'you', 'later'], 'goodbye'), (['ttyl'], 'goodbye'), (['i', 'got', 'to', 'go'], 'goodbye'), (['gtg'], 'goodbye'), (['what', 'is', 'the', 'name', 'of', 'your', 'developers'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'your', 'creators'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'the', 'developers'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'the', 'creators'], 'creator'), (['who', 'created', 'you'], 'creator'), (['your', 'developers'], 'creator'), (['your', 'creators'], 'creator'), (['who', 'are', 'your', 'developers'], 'creator'), (['developers'], 'creator'), (['you', 'are', 'made', 'by'], 'creator'), (['you', 'are', 'made', 'by', 'whom'], 'creator'), (['who', 'created', 'you'], 'creator'), (['who', 'create', 'you'], 'creator'), (['creators'], 'creator'), (['who', 'made', 'you'], 'creator'), (['who', 'designed', 'you'], 'creator'), (['name'], 'name'), (['your', 'name'], 'name'), (['do', 'you', 'have', 'a', 'name'], 'name'), (['what', 'are', 'you', 'called'], 'name'), (['what', 'is', 'your', 'name'], 'name'), (['what', 'should', 'I', 'call', 'you'], 'name'), (['whats', 'your', 'name', '?'], 'name'), (['what', 'are', 'you'], 'name'), (['who', 'are', 'you'], 'name'), (['who', 'is', 'this'], 'name'), (['what', 'am', 'i', 'chatting', 'to'], 'name'), (['who', 'am', 'i', 'taking', 'to'], 'name'), (['what', 'are', 'you'], 'name'), (['timing', 'of', 'college'], 'hours'), (['what', 'is', 'college', 'timing'], 'hours'), (['working', 'days'], 'hours'), (['when', 'are', 'you', 'guys', 'open'], 'hours'), (['what', 'are', 'your', 'hours'], 'hours'), (['hours', 'of', 'operation'], 'hours'), (['when', 'is', 'the', 'college', 'open'], 'hours'), (['college', 'timing'], 'hours'), (['what', 'about', 'college', 'timing'], 'hours'), (['is', 'college', 'open', 'on', 'saturday'], 'hours'), (['tell', 'something', 'about', 'college', 'timing'], 'hours'), (['what', 'is', 'the', 'college', 'hours'], 'hours'), (['when', 'should', 'i', 'come', 'to', 'college'], 'hours'), (['when', 'should', 'i', 'attend', 'college'], 'hours'), (['what', 'is', 'my', 'college', 'time'], 'hours'), (['college', 'timing'], 'hours'), (['timing', 'college'], 'hours'), (['more', 'info'], 'number'), (['contact', 'info'], 'number'), (['how', 'to', 'contact', 'college'], 'number'), (['college', 'telephone', 'number'], 'number'), (['college', 'number'], 'number'), (['What', 'is', 'your', 'contact', 'no'], 'number'), (['Contact', 'number', '?'], 'number'), (['how', 'to', 'call', 'you'], 'number'), (['College', 'phone', 'no', '?'], 'number'), (['how', 'can', 'i', 'contact', 'you'], 'number'), (['Can', 'i', 'get', 'your', 'phone', 'number'], 'number'), (['how', 'can', 'i', 'call', 'you'], 'number'), (['phone', 'number'], 'number'), (['phone', 'no'], 'number'), (['call'], 'number'), (['list', 'of', 'courses'], 'course'), (['list', 'of', 'courses', 'offered'], 'course'), (['list', 'of', 'courses', 'offered', 'in'], 'course'), (['what', 'are', 'the', 'courses', 'offered', 'in', 'your', 'college', '?'], 'course'), (['courses', '?'], 'course'), (['courses', 'offered'], 'course'), (['courses', 'offered', 'in', '(', 'your', 'univrsity', '(', 'UNI', ')', 'name', ')'], 'course'), (['courses', 'you', 'offer'], 'course'), (['branches', '?'], 'course'), (['courses', 'available', 'at', 'UNI', '?'], 'course'), (['branches', 'available', 'at', 'your', 'college', '?'], 'course'), (['what', 'are', 'the', 'courses', 'in', 'UNI', '?'], 'course'), (['what', 'are', 'branches', 'in', 'UNI', '?'], 'course'), (['what', 'are', 'courses', 'in', 'UNI', '?'], 'course'), (['branches', 'available', 'in', 'UNI', '?'], 'course'), (['can', 'you', 'tell', 'me', 'the', 'courses', 'available', 'in', 'UNI', '?'], 'course'), (['can', 'you', 'tell', 'me', 'the', 'branches', 'available', 'in', 'UNI', '?'], 'course'), (['computer', 'engineering', '?'], 'course'), (['computer'], 'course'), (['Computer', 'engineering', '?'], 'course'), (['it'], 'course'), (['IT'], 'course'), (['Information', 'Technology'], 'course'), (['AI/Ml'], 'course'), (['Mechanical', 'engineering'], 'course'), (['Chemical', 'engineering'], 'course'), (['Civil', 'engineering'], 'course'), (['information', 'about', 'fee'], 'fees'), (['information', 'on', 'fee'], 'fees'), (['tell', 'me', 'the', 'fee'], 'fees'), (['college', 'fee'], 'fees'), (['fee', 'per', 'semester'], 'fees'), (['what', 'is', 'the', 'fee', 'of', 'each', 'semester'], 'fees'), (['what', 'is', 'the', 'fees', 'of', 'each', 'year'], 'fees'), (['what', 'is', 'fee'], 'fees'), (['what', 'is', 'the', 'fees'], 'fees'), (['how', 'much', 'is', 'the', 'fees'], 'fees'), (['fees', 'for', 'first', 'year'], 'fees'), (['fees'], 'fees'), (['about', 'the', 'fees'], 'fees'), (['tell', 'me', 'something', 'about', 'the', 'fees'], 'fees'), (['What', 'is', 'the', 'fees', 'of', 'hostel'], 'fees'), (['how', 'much', 'is', 'the', 'fees'], 'fees'), (['hostel', 'fees'], 'fees'), (['fees', 'for', 'AC', 'room'], 'fees'), (['fees', 'for', 'non-AC', 'room'], 'fees'), (['fees', 'for', 'Ac', 'room', 'for', 'girls'], 'fees'), (['fees', 'for', 'non-Ac', 'room', 'for', 'girls'], 'fees'), (['fees', 'for', 'Ac', 'room', 'for', 'boys'], 'fees'), (['fees', 'for', 'non-Ac', 'room', 'for', 'boys'], 'fees'), (['where', 'is', 'the', 'college', 'located'], 'location'), (['college', 'is', 'located', 'at'], 'location'), (['where', 'is', 'college'], 'location'), (['where', 'is', 'college', 'located'], 'location'), (['address', 'of', 'college'], 'location'), (['how', 'to', 'reach', 'college'], 'location'), (['college', 'location'], 'location'), (['college', 'address'], 'location'), (['wheres', 'the', 'college'], 'location'), (['how', 'can', 'I', 'reach', 'college'], 'location'), (['whats', 'is', 'the', 'college', 'address'], 'location'), (['what', 'is', 'the', 'address', 'of', 'college'], 'location'), (['address'], 'location'), (['location'], 'location'), (['hostel', 'facility'], 'hostel'), (['hostel', 'servive'], 'hostel'), (['hostel', 'location'], 'hostel'), (['hostel', 'address'], 'hostel'), (['hostel', 'facilities'], 'hostel'), (['hostel', 'fees'], 'hostel'), (['Does', 'college', 'provide', 'hostel'], 'hostel'), (['Is', 'there', 'any', 'hostel'], 'hostel'), (['Where', 'is', 'hostel'], 'hostel'), (['do', 'you', 'have', 'hostel'], 'hostel'), (['do', 'you', 'guys', 'have', 'hostel'], 'hostel'), (['hostel'], 'hostel'), (['hostel', 'capacity'], 'hostel'), (['what', 'is', 'the', 'hostel', 'fee'], 'hostel'), (['how', 'to', 'get', 'in', 'hostel'], 'hostel'), (['what', 'is', 'the', 'hostel', 'address'], 'hostel'), (['how', 'far', 'is', 'hostel', 'from', 'college'], 'hostel'), (['hostel', 'college', 'distance'], 'hostel'), (['where', 'is', 'the', 'hostel'], 'hostel'), (['how', 'big', 'is', 'the', 'hostel'], 'hostel'), (['distance', 'between', 'college', 'and', 'hostel'], 'hostel'), (['distance', 'between', 'hostel', 'and', 'college'], 'hostel'), (['events', 'organised'], 'event'), (['list', 'of', 'events'], 'event'), (['list', 'of', 'events', 'organised', 'in', 'college'], 'event'), (['list', 'of', 'events', 'conducted', 'in', 'college'], 'event'), (['What', 'events', 'are', 'conducted', 'in', 'college'], 'event'), (['Are', 'there', 'any', 'event', 'held', 'at', 'college'], 'event'), (['Events', '?'], 'event'), (['functions'], 'event'), (['what', 'are', 'the', 'events'], 'event'), (['tell', 'me', 'about', 'events'], 'event'), (['what', 'about', 'events'], 'event'), (['document', 'to', 'bring'], 'document'), (['documents', 'needed', 'for', 'admision'], 'document'), (['documents', 'needed', 'at', 'the', 'time', 'of', 'admission'], 'document'), (['documents', 'needed', 'during', 'admission'], 'document'), (['documents', 'required', 'for', 'admision'], 'document'), (['documents', 'required', 'at', 'the', 'time', 'of', 'admission'], 'document'), (['documents', 'required', 'during', 'admission'], 'document'), (['What', 'document', 'are', 'required', 'for', 'admission'], 'document'), (['Which', 'document', 'to', 'bring', 'for', 'admission'], 'document'), (['documents'], 'document'), (['what', 'documents', 'do', 'i', 'need'], 'document'), (['what', 'documents', 'do', 'I', 'need', 'for', 'admission'], 'document'), (['documents', 'needed'], 'document'), (['size', 'of', 'campus'], 'floors'), (['building', 'size'], 'floors'), (['How', 'many', 'floors', 'does', 'college', 'have'], 'floors'), (['floors', 'in', 'college'], 'floors'), (['floors', 'in', 'college'], 'floors'), (['how', 'tall', 'is', 'UNI', \"'s\", 'College', 'of', 'Engineering', 'college', 'building'], 'floors'), (['floors'], 'floors'), (['Syllabus', 'for', 'IT'], 'syllabus'), (['what', 'is', 'the', 'Information', 'Technology', 'syllabus'], 'syllabus'), (['syllabus'], 'syllabus'), (['timetable'], 'syllabus'), (['what', 'is', 'IT', 'syllabus'], 'syllabus'), (['syllabus'], 'syllabus'), (['What', 'is', 'next', 'lecture'], 'syllabus'), (['is', 'there', 'any', 'library'], 'library'), (['library', 'facility'], 'library'), (['library', 'facilities'], 'library'), (['do', 'you', 'have', 'library'], 'library'), (['does', 'the', 'college', 'have', 'library', 'facility'], 'library'), (['college', 'library'], 'library'), (['where', 'can', 'i', 'get', 'books'], 'library'), (['book', 'facility'], 'library'), (['Where', 'is', 'library'], 'library'), (['Library'], 'library'), (['Library', 'information'], 'library'), (['Library', 'books', 'information'], 'library'), (['Tell', 'me', 'about', 'library'], 'library'), (['how', 'many', 'libraries'], 'library'), (['how', 'is', 'college', 'infrastructure'], 'infrastructure'), (['infrastructure'], 'infrastructure'), (['college', 'infrastructure'], 'infrastructure'), (['food', 'facilities'], 'canteen'), (['canteen', 'facilities'], 'canteen'), (['canteen', 'facility'], 'canteen'), (['is', 'there', 'any', 'canteen'], 'canteen'), (['Is', 'there', 'a', 'cafetaria', 'in', 'college'], 'canteen'), (['Does', 'college', 'have', 'canteen'], 'canteen'), (['Where', 'is', 'canteen'], 'canteen'), (['where', 'is', 'cafetaria'], 'canteen'), (['canteen'], 'canteen'), (['Food'], 'canteen'), (['Cafetaria'], 'canteen'), (['food', 'menu'], 'menu'), (['food', 'in', 'canteen'], 'menu'), (['Whats', 'there', 'on', 'menu'], 'menu'), (['what', 'is', 'available', 'in', 'college', 'canteen'], 'menu'), (['what', 'foods', 'can', 'we', 'get', 'in', 'college', 'canteen'], 'menu'), (['food', 'variety'], 'menu'), (['What', 'is', 'there', 'to', 'eat', '?'], 'menu'), (['What', 'is', 'college', 'placement'], 'placement'), (['Which', 'companies', 'visit', 'in', 'college'], 'placement'), (['What', 'is', 'average', 'package'], 'placement'), (['companies', 'visit'], 'placement'), (['package'], 'placement'), (['About', 'placement'], 'placement'), (['placement'], 'placement'), (['recruitment'], 'placement'), (['companies'], 'placement'), (['Who', 'is', 'HOD'], 'ithod'), (['Where', 'is', 'HOD'], 'ithod'), (['it', 'hod'], 'ithod'), (['name', 'of', 'it', 'hod'], 'ithod'), (['Who', 'is', 'computer', 'HOD'], 'computerhod'), (['Where', 'is', 'computer', 'HOD'], 'computerhod'), (['computer', 'hod'], 'computerhod'), (['name', 'of', 'computer', 'hod'], 'computerhod'), (['Who', 'is', 'extc', 'HOD'], 'extchod'), (['Where', 'is', 'extc', 'HOD'], 'extchod'), (['extc', 'hod'], 'extchod'), (['name', 'of', 'extc', 'hod'], 'extchod'), (['what', 'is', 'the', 'name', 'of', 'principal'], 'principal'), (['whatv', 'is', 'the', 'principal', 'name'], 'principal'), (['principal', 'name'], 'principal'), (['Who', 'is', 'college', 'principal'], 'principal'), (['Where', 'is', 'principal', \"'s\", 'office'], 'principal'), (['principal'], 'principal'), (['name', 'of', 'principal'], 'principal'), (['exam', 'dates'], 'sem'), (['exam', 'schedule'], 'sem'), (['When', 'is', 'semester', 'exam'], 'sem'), (['Semester', 'exam', 'timetable'], 'sem'), (['sem'], 'sem'), (['semester'], 'sem'), (['exam'], 'sem'), (['when', 'is', 'exam'], 'sem'), (['exam', 'timetable'], 'sem'), (['exam', 'dates'], 'sem'), (['when', 'is', 'semester'], 'sem'), (['what', 'is', 'the', 'process', 'of', 'admission'], 'admission'), (['what', 'is', 'the', 'admission', 'process'], 'admission'), (['How', 'to', 'take', 'admission', 'in', 'your', 'college'], 'admission'), (['What', 'is', 'the', 'process', 'for', 'admission'], 'admission'), (['admission'], 'admission'), (['admission', 'process'], 'admission'), (['scholarship'], 'scholarship'), (['Is', 'scholarship', 'available'], 'scholarship'), (['scholarship', 'engineering'], 'scholarship'), (['scholarship', 'it'], 'scholarship'), (['scholarship', 'ce'], 'scholarship'), (['scholarship', 'mechanical'], 'scholarship'), (['scholarship', 'civil'], 'scholarship'), (['scholarship', 'chemical'], 'scholarship'), (['scholarship', 'for', 'AI/ML'], 'scholarship'), (['available', 'scholarships'], 'scholarship'), (['scholarship', 'for', 'computer', 'engineering'], 'scholarship'), (['scholarship', 'for', 'IT', 'engineering'], 'scholarship'), (['scholarship', 'for', 'mechanical', 'engineering'], 'scholarship'), (['scholarship', 'for', 'civil', 'engineering'], 'scholarship'), (['scholarship', 'for', 'chemical', 'engineering'], 'scholarship'), (['list', 'of', 'scholarship'], 'scholarship'), (['comps', 'scholarship'], 'scholarship'), (['IT', 'scholarship'], 'scholarship'), (['mechanical', 'scholarship'], 'scholarship'), (['civil', 'scholarship'], 'scholarship'), (['chemical', 'scholarship'], 'scholarship'), (['automobile', 'scholarship'], 'scholarship'), (['first', 'year', 'scholarship'], 'scholarship'), (['second', 'year', 'scholarship'], 'scholarship'), (['third', 'year', 'scholarship'], 'scholarship'), (['fourth', 'year', 'scholarship'], 'scholarship'), (['What', 'facilities', 'college', 'provide'], 'facilities'), (['College', 'facility'], 'facilities'), (['What', 'are', 'college', 'facilities'], 'facilities'), (['facilities'], 'facilities'), (['facilities', 'provided'], 'facilities'), (['max', 'number', 'of', 'students'], 'college intake'), (['number', 'of', 'seats', 'per', 'branch'], 'college intake'), (['number', 'of', 'seats', 'in', 'each', 'branch'], 'college intake'), (['maximum', 'number', 'of', 'seats'], 'college intake'), (['maximum', 'students', 'intake'], 'college intake'), (['What', 'is', 'college', 'intake'], 'college intake'), (['how', 'many', 'stundent', 'are', 'taken', 'in', 'each', 'branch'], 'college intake'), (['seat', 'allotment'], 'college intake'), (['seats'], 'college intake'), (['college', 'dress', 'code'], 'uniform'), (['college', 'dresscode'], 'uniform'), (['what', 'is', 'the', 'uniform'], 'uniform'), (['can', 'we', 'wear', 'casuals'], 'uniform'), (['Does', 'college', 'have', 'an', 'uniform'], 'uniform'), (['Is', 'there', 'any', 'uniform'], 'uniform'), (['uniform'], 'uniform'), (['what', 'about', 'uniform'], 'uniform'), (['do', 'we', 'have', 'to', 'wear', 'uniform'], 'uniform'), (['what', 'are', 'the', 'different', 'committe', 'in', 'college'], 'committee'), (['different', 'committee', 'in', 'college'], 'committee'), (['Are', 'there', 'any', 'committee', 'in', 'college'], 'committee'), (['Give', 'me', 'committee', 'details'], 'committee'), (['committee'], 'committee'), (['how', 'many', 'committee', 'are', 'there', 'in', 'college'], 'committee'), (['I', 'love', 'you'], 'random'), (['Will', 'you', 'marry', 'me'], 'random'), (['Do', 'you', 'love', 'me'], 'random'), (['fuck'], 'swear'), (['bitch'], 'swear'), (['shut', 'up'], 'swear'), (['hell'], 'swear'), (['stupid'], 'swear'), (['idiot'], 'swear'), (['dumb', 'ass'], 'swear'), (['asshole'], 'swear'), (['fucker'], 'swear'), (['holidays'], 'vacation'), (['when', 'will', 'semester', 'starts'], 'vacation'), (['when', 'will', 'semester', 'end'], 'vacation'), (['when', 'is', 'the', 'holidays'], 'vacation'), (['list', 'of', 'holidays'], 'vacation'), (['Holiday', 'in', 'these', 'year'], 'vacation'), (['holiday', 'list'], 'vacation'), (['about', 'vacations'], 'vacation'), (['about', 'holidays'], 'vacation'), (['When', 'is', 'vacation'], 'vacation'), (['When', 'is', 'holidays'], 'vacation'), (['how', 'long', 'will', 'be', 'the', 'vacation'], 'vacation'), (['sports', 'and', 'games'], 'sports'), (['give', 'sports', 'details'], 'sports'), (['sports', 'infrastructure'], 'sports'), (['sports', 'facilities'], 'sports'), (['information', 'about', 'sports'], 'sports'), (['Sports', 'activities'], 'sports'), (['please', 'provide', 'sports', 'and', 'games', 'information'], 'sports'), (['okk'], 'salutaion'), (['okie'], 'salutaion'), (['nice', 'work'], 'salutaion'), (['well', 'done'], 'salutaion'), (['good', 'job'], 'salutaion'), (['thanks', 'for', 'the', 'help'], 'salutaion'), (['Thank', 'You'], 'salutaion'), (['its', 'ok'], 'salutaion'), (['Thanks'], 'salutaion'), (['Good', 'work'], 'salutaion'), (['k'], 'salutaion'), (['ok'], 'salutaion'), (['okay'], 'salutaion'), (['what', 'can', 'you', 'do'], 'task'), (['what', 'are', 'the', 'thing', 'you', 'can', 'do'], 'task'), (['things', 'you', 'can', 'do'], 'task'), (['what', 'can', 'u', 'do', 'for', 'me'], 'task'), (['how', 'u', 'can', 'help', 'me'], 'task'), (['why', 'i', 'should', 'use', 'you'], 'task'), (['ragging'], 'ragging'), (['is', 'ragging', 'practice', 'active', 'in', 'college'], 'ragging'), (['does', 'college', 'have', 'any', 'antiragging', 'facility'], 'ragging'), (['is', 'there', 'any', 'ragging', 'cases'], 'ragging'), (['is', 'ragging', 'done', 'here'], 'ragging'), (['ragging', 'against'], 'ragging'), (['antiragging', 'facility'], 'ragging'), (['ragging', 'juniors'], 'ragging'), (['ragging', 'history'], 'ragging'), (['ragging', 'incidents'], 'ragging'), (['hod'], 'hod'), (['hod', 'name'], 'hod'), (['who', 'is', 'the', 'hod'], 'hod')] \n",
            "\n",
            "38 classes\n",
            " ['admission', 'canteen', 'college intake', 'committee', 'computerhod', 'course', 'creator', 'document', 'event', 'extchod', 'facilities', 'fees', 'floors', 'goodbye', 'greeting', 'hod', 'hostel', 'hours', 'infrastructure', 'ithod', 'library', 'location', 'menu', 'name', 'number', 'placement', 'principal', 'ragging', 'random', 'salutaion', 'scholarship', 'sem', 'sports', 'swear', 'syllabus', 'task', 'uniform', 'vacation'] \n",
            "\n",
            "263 unique lemmatized words\n",
            " [\"'s\", '(', ')', 'a', 'about', 'ac', 'active', 'activity', 'address', 'admision', 'admission', 'against', 'ai/ml', 'allotment', 'am', 'an', 'and', 'antiragging', 'any', 'anyone', 'are', 'as', 'asshole', 'at', 'attend', 'automobile', 'available', 'average', 'be', 'between', 'big', 'bitch', 'book', 'boy', 'branch', 'bring', 'building', 'by', 'bye', 'cafetaria', 'call', 'called', 'campus', 'can', 'canteen', 'capacity', 'case', 'casuals', 'ce', 'chatting', 'chemical', 'civil', 'code', 'college', 'come', 'committe', 'committee', 'comp', 'company', 'computer', 'conducted', 'contact', 'course', 'create', 'created', 'creator', 'cya', 'date', 'day', 'designed', 'detail', 'developer', 'different', 'distance', 'do', 'document', 'doe', 'done', 'dress', 'dresscode', 'dumb', 'during', 'each', 'eat', 'end', 'engineering', 'event', 'exam', 'extc', 'facility', 'far', 'fee', 'first', 'floor', 'food', 'for', 'fourth', 'from', 'fuck', 'fucker', 'function', 'game', 'get', 'girl', 'give', 'go', 'good', 'goodbye', 'got', 'gtg', 'guy', 'have', 'held', 'hell', 'hello', 'help', 'here', 'heyy', 'hi', 'history', 'hod', 'holiday', 'hostel', 'hour', 'how', 'i', 'idiot', 'in', 'incident', 'info', 'information', 'infrastructure', 'intake', 'is', 'it', 'job', 'junior', 'k', 'later', 'leaving', 'lecture', 'library', 'list', 'located', 'location', 'long', 'love', 'made', 'many', 'marry', 'max', 'maximum', 'me', 'mechanical', 'menu', 'more', 'much', 'my', 'name', 'need', 'needed', 'next', 'nice', 'no', 'non-ac', 'number', 'of', 'offer', 'offered', 'office', 'ok', 'okay', 'okie', 'okk', 'on', 'open', 'operation', 'organised', 'package', 'per', 'phone', 'placement', 'please', 'practice', 'principal', 'process', 'provide', 'provided', 'ragging', 'reach', 'recruitment', 'required', 'room', 'saturday', 'schedule', 'scholarship', 'seat', 'second', 'see', 'sem', 'semester', 'servive', 'should', 'shut', 'size', 'something', 'sport', 'start', 'student', 'stundent', 'stupid', 'syllabus', 'take', 'taken', 'taking', 'talk', 'tall', 'technology', 'telephone', 'tell', 'thank', 'thanks', 'the', 'there', 'these', 'thing', 'third', 'this', 'time', 'timetable', 'timing', 'to', 'ttyl', 'u', 'uni', 'uniform', 'univrsity', 'up', 'use', 'vacation', 'variety', 'visit', 'we', 'wear', 'well', 'what', 'whats', 'whatsup', 'whatv', 'when', 'where', 'wheres', 'which', 'who', 'whom', 'why', 'will', 'work', 'working', 'ya', 'year', 'you', 'your'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# lemmatize, lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\\n\", documents, \"\\n\")\n",
        "# classes = intents[tag]\n",
        "print (len(classes), \"classes\\n\", classes, \"\\n\")\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\\n\", words, \"\\n\")\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "id": "ea121ce6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a818b1a"
      },
      "source": [
        "# **Training Model**"
      ],
      "id": "4a818b1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230e303f"
      },
      "source": [
        "Now, we will create the training data in which we will provide the input and the output.\n",
        "- Our input will be the pattern and output will be the class our input pattern belongs to. But the computer doesn’t understand text so we will convert text into numbers"
      ],
      "id": "230e303f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3df0178",
        "outputId": "9317997f-2088-492d-da89-dc1b6c1b9d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n"
          ]
        }
      ],
      "source": [
        "# create our training data\n",
        "training = []\n",
        "test=[]\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words\n",
        "    pattern_words = doc[0]\n",
        "    # convert pattern_words in lower case\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create bag of words array,if word match found in current pattern then put 1 otherwise 0.[row * colm(263)]\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # in output array 0 value for each tag ang 1 value for matched tag.[row * colm(8)]\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "# shuffle training and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test. X - patterns(words), Y - intents(tags)\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "test_x = list(training[:,0])\n",
        "test_y = list(training[:,1])\n",
        "print(\"Training data created\")"
      ],
      "id": "e3df0178"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f55abcf8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ],
      "id": "f55abcf8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e26e37eb"
      },
      "source": [
        "# **Build the model**"
      ],
      "id": "e26e37eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "154c186a"
      },
      "source": [
        "We have our training data ready, now we will build a deep neural network that has 3 layers. We use the Keras sequential API for this. After training the model for 200 epochs, we achieved 100% accuracy on our model. Let us save the model as ‘chatbot_model.h5'."
      ],
      "id": "154c186a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b452b16",
        "outputId": "cd768eab-7fa9-4cd7-e448-489c2ff4a43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First layer: [[ 0.01171468  0.01953869 -0.07908219 ...  0.04838526 -0.10450728\n",
            "   0.05605504]\n",
            " [-0.01870174  0.06083842  0.07469876 ...  0.07183325  0.09987801\n",
            "  -0.09308451]\n",
            " [ 0.09441049 -0.0833907   0.07526898 ... -0.06627359  0.00697224\n",
            "  -0.01480728]\n",
            " ...\n",
            " [-0.060802    0.0502204   0.11968203 ... -0.08253083  0.10903965\n",
            "  -0.04468251]\n",
            " [-0.03709535 -0.08449052  0.04995953 ...  0.00351973 -0.03962933\n",
            "  -0.04021084]\n",
            " [ 0.06873345 -0.0128224  -0.06130228 ...  0.08963627 -0.00256503\n",
            "  -0.03829072]]\n"
          ]
        }
      ],
      "source": [
        "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
        "# equal to number of intents to predict output intent with softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "print(\"First layer:\",model.layers[0].get_weights()[0])"
      ],
      "id": "1b452b16"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ada84d"
      },
      "outputs": [],
      "source": [
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "id": "b4ada84d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f836664a",
        "outputId": "70146fb6-2443-43d3-b550-1f1856c8fadd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "81/81 [==============================] - 1s 2ms/step - loss: 3.6040 - accuracy: 0.0494\n",
            "Epoch 2/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 3.4542 - accuracy: 0.0963\n",
            "Epoch 3/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 3.2616 - accuracy: 0.1506\n",
            "Epoch 4/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 3.0276 - accuracy: 0.2074\n",
            "Epoch 5/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.7854 - accuracy: 0.2815\n",
            "Epoch 6/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.5168 - accuracy: 0.3383\n",
            "Epoch 7/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.3348 - accuracy: 0.3951\n",
            "Epoch 8/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.0656 - accuracy: 0.4988\n",
            "Epoch 9/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.9055 - accuracy: 0.5037\n",
            "Epoch 10/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.6606 - accuracy: 0.5654\n",
            "Epoch 11/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.4878 - accuracy: 0.6444\n",
            "Epoch 12/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3818 - accuracy: 0.6543\n",
            "Epoch 13/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2647 - accuracy: 0.6864\n",
            "Epoch 14/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1617 - accuracy: 0.6938\n",
            "Epoch 15/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0527 - accuracy: 0.7432\n",
            "Epoch 16/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9955 - accuracy: 0.7654\n",
            "Epoch 17/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.7630\n",
            "Epoch 18/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8338 - accuracy: 0.7926\n",
            "Epoch 19/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8403 - accuracy: 0.7580\n",
            "Epoch 20/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.7347 - accuracy: 0.8074\n",
            "Epoch 21/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.7600 - accuracy: 0.8173\n",
            "Epoch 22/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.8469\n",
            "Epoch 23/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.6151 - accuracy: 0.8321\n",
            "Epoch 24/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.8543\n",
            "Epoch 25/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.8593\n",
            "Epoch 26/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.8741\n",
            "Epoch 27/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8741\n",
            "Epoch 28/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.8593\n",
            "Epoch 29/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8716\n",
            "Epoch 30/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8716\n",
            "Epoch 31/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8938\n",
            "Epoch 32/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.9012\n",
            "Epoch 33/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8840\n",
            "Epoch 34/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.9235\n",
            "Epoch 35/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8889\n",
            "Epoch 36/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.9235\n",
            "Epoch 37/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8914\n",
            "Epoch 38/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8815\n",
            "Epoch 39/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.9111\n",
            "Epoch 40/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.9111\n",
            "Epoch 41/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.9383\n",
            "Epoch 42/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.9358\n",
            "Epoch 43/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.9086\n",
            "Epoch 44/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9309\n",
            "Epoch 45/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9358\n",
            "Epoch 46/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.9259\n",
            "Epoch 47/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.9407\n",
            "Epoch 48/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.9457\n",
            "Epoch 49/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9407\n",
            "Epoch 50/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9284\n",
            "Epoch 51/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9383\n",
            "Epoch 52/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9407\n",
            "Epoch 53/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9432\n",
            "Epoch 54/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9457\n",
            "Epoch 55/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9160\n",
            "Epoch 56/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1911 - accuracy: 0.9407\n",
            "Epoch 57/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9383\n",
            "Epoch 58/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.9481\n",
            "Epoch 59/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9506\n",
            "Epoch 60/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1910 - accuracy: 0.9383\n",
            "Epoch 61/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2196 - accuracy: 0.9407\n",
            "Epoch 62/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9630\n",
            "Epoch 63/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.9556\n",
            "Epoch 64/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1971 - accuracy: 0.9284\n",
            "Epoch 65/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9481\n",
            "Epoch 66/75\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9506\n",
            "Epoch 67/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9358\n",
            "Epoch 68/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9877\n",
            "Epoch 69/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.9457\n",
            "Epoch 70/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9580\n",
            "Epoch 71/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1436 - accuracy: 0.9630\n",
            "Epoch 72/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9457\n",
            "Epoch 73/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9506\n",
            "Epoch 74/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1212 - accuracy: 0.9679\n",
            "Epoch 75/75\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9704\n",
            "model created\n"
          ]
        }
      ],
      "source": [
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=75, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model created\")\n"
      ],
      "id": "f836664a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8r4MdkyChYz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=42)\n"
      ],
      "id": "P8r4MdkyChYz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T2Acbg9Cjd1",
        "outputId": "e32cd23a-b000-46aa-89f7-bde6c2bac208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9975\n",
            "Test loss: 0.013823220506310463\n",
            "Test accuracy: 0.9975308775901794\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(np.array(test_x), np.array(test_y), verbose=1)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "id": "8T2Acbg9Cjd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653397e1"
      },
      "source": [
        "# FOR PREDICTING RESPONSE\n"
      ],
      "id": "653397e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54VmWF052_DD"
      },
      "outputs": [],
      "source": [
        "intents = json.loads(open('intents (1).json').read())\n",
        "\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ],
      "id": "54VmWF052_DD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MowgJNCmUOl3"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "\n",
        "# tokenize the pattern - split words into array\n",
        "\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "#print(sentence_words)\n",
        "# stem each word - create short form for word\n",
        "\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "#print(sentence_words)\n",
        "\n",
        "  return sentence_words"
      ],
      "id": "MowgJNCmUOl3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLWInpSYUhwf"
      },
      "outputs": [],
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "\n",
        "# tokenize the pattern\n",
        "\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  #print(sentence_words)\n",
        "\n",
        "  # bag of words - matrix of N words, vocabulary matrix\n",
        "\n",
        "  bag = [0]*len(words)\n",
        "  #print(bag)\n",
        "\n",
        "  for s in sentence_words:\n",
        "      for i,w in enumerate(words):\n",
        "          if w == s:\n",
        "              # assign 1 if current word is in the vocabulary position\n",
        "              bag[i] = 1\n",
        "              if show_details:\n",
        "                  print (\"found in bag: %s\" % w)\n",
        "              #print (\"found in bag: %s\" % w)\n",
        "  #print(bag)\n",
        "  return(np.array(bag))"
      ],
      "id": "xLWInpSYUhwf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmvPJ1j6U1ag"
      },
      "outputs": [],
      "source": [
        "def predict_class(sentence, model):\n",
        "\n",
        "  # filter out predictions below a threshold\n",
        "\n",
        "  p = bow(sentence, words,show_details=False)\n",
        "  #print(p)\n",
        "\n",
        "  res = model.predict(np.array([p]))[0]\n",
        "  #print(res)\n",
        "\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "\n",
        "  results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "  #print(results)\n",
        "  # sort by strength of probability\n",
        "\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  #print(results)\n",
        "\n",
        "  return_list = []\n",
        "\n",
        "  for r in results:\n",
        "      return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "\n",
        "  return return_list\n",
        "  #print(return_list)"
      ],
      "id": "MmvPJ1j6U1ag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQwKHEc4U-s5"
      },
      "outputs": [],
      "source": [
        "def getResponse(ints, intents_json):\n",
        "\n",
        "  tag = ints[0]['intent']\n",
        "  #print(tag)\n",
        "\n",
        "  list_of_intents = intents_json['intents']\n",
        "  #print(list_of_intents)\n",
        "\n",
        "  for i in list_of_intents:\n",
        "      if(i['tag']== tag):\n",
        "          result = random.choice(i['responses'])\n",
        "          break\n",
        "  return result"
      ],
      "id": "jQwKHEc4U-s5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R55xDGQWVE0y"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(text):\n",
        "  ints = predict_class(text, model)\n",
        "\n",
        "  #print(ints)\n",
        "\n",
        "  res = getResponse(ints, intents)\n",
        "  #print(res)\n",
        "  return res"
      ],
      "id": "R55xDGQWVE0y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhBqsqz9Vcpp"
      },
      "source": [
        "# Predicting results"
      ],
      "id": "KhBqsqz9Vcpp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLhrWj0OVk6P",
        "outputId": "bf1155c8-36c5-48aa-afa9-73961ac1b024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Message:hello\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "Hello,how can I help you today\n",
            "Enter Message:admission\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Application can also be submitted online through the University's website\n",
            "Enter Message:exit\n"
          ]
        }
      ],
      "source": [
        "start = True\n",
        "while start:\n",
        "\n",
        "  query = input('Enter Message:')\n",
        "  if query in ['quit','exit','bye']:\n",
        "      start = False\n",
        "      continue\n",
        "  try:\n",
        "      res = chatbot_response(query)\n",
        "      print(res)\n",
        "  except:\n",
        "      print('You may need to rephrase your question.')"
      ],
      "id": "tLhrWj0OVk6P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_qvIlUfEF-u",
        "outputId": "fdb36d18-a8a3-4138-b656-f69e9840ea7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2023.11.17)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gtts\n"
      ],
      "id": "K_qvIlUfEF-u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQynPPu8qzuc"
      },
      "source": [
        "## Let's Chat"
      ],
      "id": "xQynPPu8qzuc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk-9-Q5ARAaa",
        "outputId": "b849eb15-92a4-44af-bba2-5efcdf7890b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Message:exit\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Sentence tokenizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Your existing code...\n",
        "\n",
        "start = True\n",
        "while start:\n",
        "    query = input('Enter Message:')\n",
        "    if query.lower() in ['quit', 'exit', 'bye']:\n",
        "        start = False\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        res = chatbot_response(query)\n",
        "        print(res)\n",
        "\n",
        "        # Convert the response to speech\n",
        "        tts = gTTS(text=res, lang='en')\n",
        "        tts.save(\"response.mp3\")\n",
        "\n",
        "        # Play the response\n",
        "        display(Audio(filename=\"response.mp3\", autoplay=True))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n"
      ],
      "id": "zk-9-Q5ARAaa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OUPcN1YHuP8",
        "outputId": "15aed5ff-95eb-4b55-e3ba-92ea3ed05b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.11.17)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition\n"
      ],
      "id": "0OUPcN1YHuP8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-5C8ShxIO_i",
        "outputId": "b74111b8-58ca-4f1b-8f86-0627d6f9bb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libasound2-dev' instead of 'libasound-dev'\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 0s (815 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0\n"
      ],
      "id": "A-5C8ShxIO_i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9WWPCx3IF-v",
        "outputId": "d33303d0-6fed-4086-b8f6-72b4c49c0b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=PyAudio-0.2.14-cp310-cp310-linux_x86_64.whl size=63855 sha256=c171feeb892f38fdf2964d6a83d76e2804c62f3cca595edcfe863b38a282f4d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/21/f4/0b51d41ba79e51b16295cbb096ec49f334792814d545b508c5\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.14\n"
          ]
        }
      ],
      "source": [
        "!pip install pyaudio\n"
      ],
      "id": "E9WWPCx3IF-v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXQNSmvcI6xx",
        "outputId": "5ce58e27-f34c-4292-afe1-f1272d1e453f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "# Print the list of available audio input devices and their indices\n",
        "print(sr.Microphone.list_microphone_names())\n"
      ],
      "id": "XXQNSmvcI6xx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M-WINQdR-C3",
        "outputId": "109721c0-634a-4eca-be54-49a3d9989659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: gtts in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.31.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2023.11.17)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk numpy tensorflow keras gtts SpeechRecognition\n"
      ],
      "id": "4M-WINQdR-C3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO3EusFGUCuy"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import json\n",
        "# import pickle\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Activation, Dropout\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "# import random\n",
        "# from gtts import gTTS\n",
        "# from IPython.display import Audio, display\n",
        "# import speech_recognition as sr\n",
        "\n",
        "# # Your existing code...\n",
        "\n",
        "# def get_voice_input():\n",
        "#     recognizer = sr.Recognizer()\n",
        "#     microphone = sr.Microphone()\n",
        "\n",
        "#     with microphone as source:\n",
        "#         print(\"Say something:\")\n",
        "#         recognizer.adjust_for_ambient_noise(source)\n",
        "#         audio = recognizer.listen(source, timeout=5)\n",
        "\n",
        "#     try:\n",
        "#         text = recognizer.recognize_google(audio)\n",
        "#         print(\"You said:\", text)\n",
        "\n",
        "#         return text\n",
        "\n",
        "#     except sr.UnknownValueError:\n",
        "#         print(\"Could not understand audio.\")\n",
        "#         return None\n",
        "#     except sr.RequestError as e:\n",
        "#         print(f\"Error connecting to Google API: {e}\")\n",
        "#         return None\n",
        "\n",
        "# start = True\n",
        "# while start:\n",
        "#     query_audio = get_voice_input()\n",
        "\n",
        "#     if query_audio:\n",
        "#         try:\n",
        "#             res = chatbot_response(query_audio)\n",
        "#             print(res)\n",
        "\n",
        "#             # Convert the response to speech\n",
        "#             tts = gTTS(text=res, lang='en')\n",
        "#             tts.save(\"response.mp3\")\n",
        "\n",
        "#             # Play the response\n",
        "#             display(Audio(filename=\"response.mp3\", autoplay=True))\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f'Error: {e}')\n",
        "\n",
        "#     else:\n",
        "#         print(\"No audio input. Please try again.\")\n"
      ],
      "id": "AO3EusFGUCuy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-J7VpfAThzk",
        "outputId": "d89397de-5c57-4cdc-c39d-a38f055f622f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg-python"
      ],
      "id": "8-J7VpfAThzk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc5FgkdAfMlP",
        "outputId": "7c42429a-515a-45a0-95fa-ef5af44268b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n"
      ],
      "id": "qc5FgkdAfMlP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24_nZMVHU4NQ",
        "outputId": "d5974648-cb50-48f2-fa89-46fc83f3a76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition pydub\n"
      ],
      "id": "24_nZMVHU4NQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYn9CkKHroHu"
      },
      "source": [
        "## Code for input of Audio from the Microphone using HTML and JavaScript"
      ],
      "id": "KYn9CkKHroHu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH2AmWzWgMrJ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import time\n",
        "import io\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import ffmpeg\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "  };\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  resolve(base64data.toString())\n",
        "});\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "id": "bH2AmWzWgMrJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxHwOBVbr1iL"
      },
      "source": [
        "## Chat through System Audio"
      ],
      "id": "rxHwOBVbr1iL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc3MK9R7sA8b"
      },
      "source": [
        "## Run the Below Code to get the Response for the System Input from the Bot"
      ],
      "id": "gc3MK9R7sA8b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "QNX4rKEMi7ut",
        "outputId": "3fec2703-3beb-4b6c-e482-3c8fb48e0d0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "  };\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {\n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data);\n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "};\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  resolve(base64data.toString())\n",
              "});\n",
              "}\n",
              "});\n",
              "\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: cannot use a string pattern on a bytes-like object\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_audio():\n",
        "    display(HTML(AUDIO_HTML))\n",
        "    data = eval_js(\"data\")\n",
        "\n",
        "    if not data:\n",
        "        return None, None\n",
        "\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "\n",
        "    # Convert audio data to pydub AudioSegment\n",
        "    audio_segment = AudioSegment.from_file(io.BytesIO(binary), format=\"webm\")\n",
        "\n",
        "    # Convert to mono channel\n",
        "    audio_segment = audio_segment.set_channels(1)\n",
        "\n",
        "    # Convert to 16-bit PCM format\n",
        "    audio_segment = audio_segment.set_sample_width(2)\n",
        "\n",
        "    # Convert to 16 kHz sample rate\n",
        "    audio_segment = audio_segment.set_frame_rate(16000)\n",
        "\n",
        "    # Save the processed audio to a file\n",
        "    audio_segment.export(\"audio.wav\", format=\"wav\")\n",
        "\n",
        "    # Read the processed audio file\n",
        "    sr, audio = wav_read(\"audio.wav\")\n",
        "\n",
        "    return audio, sr\n",
        "\n",
        "start = True\n",
        "while start:\n",
        "    query_audio, _ = get_audio()\n",
        "\n",
        "    if query_audio is not None:\n",
        "        try:\n",
        "            res = chatbot_response(query_audio)\n",
        "            print(res)\n",
        "\n",
        "            # Convert the response to speech\n",
        "            tts = gTTS(text=res, lang='en')\n",
        "            tts.save(\"response.mp3\")\n",
        "\n",
        "            # Play the response\n",
        "            display(Audio(filename=\"response.mp3\", autoplay=True))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Error: {e}')\n",
        "\n",
        "    # Add a delay after processing to prevent continuous recording\n",
        "    time.sleep(5)  # You can adjust the delay as needed\n",
        "    start = False  # Stop the recording loop after the first iteration"
      ],
      "id": "QNX4rKEMi7ut"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4DBszW3ng07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "d8541096-0241-4d25-d132-4903a5c4b937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n",
            "You can call me Mind Reader.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def speech_to_text(audio_file):\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        audio = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio.\")\n",
        "        return None\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Error connecting to Google API: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get the text from \"audio.wav\"\n",
        "audio_text = speech_to_text(\"audio.wav\")\n",
        "\n",
        "# Ask the bot using the detected text\n",
        "if audio_text:\n",
        "    try:\n",
        "        res = chatbot_response(audio_text)\n",
        "        print(res)\n",
        "\n",
        "        # Convert the response to speech\n",
        "        tts = gTTS(text=res, lang='en')\n",
        "        tts.save(\"response.mp3\")\n",
        "\n",
        "        # Play the response\n",
        "        display(Audio(filename=\"response.mp3\", autoplay=True))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n"
      ],
      "id": "S4DBszW3ng07"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 65.361653,
      "end_time": "2023-02-23T05:13:25.834630",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-02-23T05:12:20.472977",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}